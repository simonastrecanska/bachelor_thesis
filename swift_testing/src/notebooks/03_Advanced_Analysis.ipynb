{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Analysis\n",
    "\n",
    "This notebook extends the evaluation of your SWIFT routing testing framework.\n",
    "\n",
    ":::warning Note\n",
    "This notebook assumes that the merged DataFrame (`merged_df`) is available from the Basic Analytics notebook. If not, the code below will attempt to reload the data.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, matthews_corrcoef, cohen_kappa_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Retrieve connection string and recreate engine\n",
    "%store -r db_uri\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine(db_uri)\n",
    "print(\"Engine recreated from stored connection string.\")\n",
    "\n",
    "# Retrieve merged_df from Basic Analytics notebook\n",
    "%store -r merged_df\n",
    "\n",
    "try:\n",
    "    merged_df\n",
    "    print(\"merged_df is available.\")\n",
    "except NameError:\n",
    "    print(\"merged_df is not defined. Reloading from database...\")\n",
    "    import pandas as pd\n",
    "    expected_df = pd.read_sql(\"SELECT * FROM expected_results\", engine)\n",
    "    actual_df = pd.read_sql(\"SELECT * FROM actual_results\", engine)\n",
    "    print(\"Merged DataFrame recreated from the database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Metrics and Visualizations\n",
    "\n",
    "Below we compute advanced evaluation metrics. In this example, we demonstrate ROC curve and calibration plot generation, as well as computing the Matthews Correlation Coefficient and Cohen's Kappa. This demonstration assumes a binary classification scenario; if you have more classes, consider one-vs-rest methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, matthews_corrcoef, cohen_kappa_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "unique_labels = merged_df['expected_label'].str.upper().unique()\n",
    "\n",
    "if len(unique_labels) == 2:\n",
    "    sorted_labels = sorted(unique_labels)\n",
    "    pos_label = sorted_labels[1]\n",
    "    \n",
    "    y_true = (merged_df['expected_label'].str.upper() == pos_label).astype(int)\n",
    "    \n",
    "    y_score = merged_df['confidence']\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:0.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_score, n_bins=10)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(prob_pred, prob_true, marker='o', linewidth=1, label='Calibration curve')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "    plt.xlabel('Mean predicted probability')\n",
    "    plt.ylabel('Fraction of positives')\n",
    "    plt.title('Calibration Plot')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    y_pred_binary = (merged_df['predicted_label'].str.upper() == pos_label).astype(int)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred_binary)\n",
    "    kappa = cohen_kappa_score(merged_df['expected_label'].str.upper(), \n",
    "                              merged_df['predicted_label'].str.upper())\n",
    "    \n",
    "    print(f\"Matthews Correlation Coefficient (MCC): {mcc:0.2f}\")\n",
    "    print(f\"Cohen's Kappa: {kappa:0.2f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Multiclass scenario detected. Computing one-vs-rest ROC curves.\")\n",
    "    \n",
    "    unique_labels = np.sort(merged_df['expected_label'].str.upper().unique())\n",
    "    n_classes = len(unique_labels)\n",
    "    \n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    y_true_multi = label_binarize(merged_df['expected_label'].str.upper(), classes=unique_labels)\n",
    "    \n",
    "    y_score_multi = np.zeros((merged_df.shape[0], n_classes))\n",
    "    for i, row in merged_df.iterrows():\n",
    "        pred_class = row['predicted_label'].upper()\n",
    "        # Get the index of the predicted class in unique_labels\n",
    "        if pred_class in unique_labels:\n",
    "            class_idx = list(unique_labels).index(pred_class)\n",
    "            y_score_multi[i, class_idx] = row['confidence']\n",
    "    \n",
    "    # Compute ROC curve and AUC for each class\n",
    "    plt.figure(figsize=(8,6))\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    for i in range(n_classes):\n",
    "        fpr, tpr, _ = roc_curve(y_true_multi[:, i], y_score_multi[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'ROC for {unique_labels[i]} (AUC = {roc_auc:0.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('One-vs-Rest ROC Curves for Multiclass Classification')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Calibration plot not implemented for multiclass scenario in this example.\")\n",
    "    \n",
    "    mcc = matthews_corrcoef(merged_df['expected_label'].str.upper(), \n",
    "                             merged_df['predicted_label'].str.upper())\n",
    "    kappa = cohen_kappa_score(merged_df['expected_label'].str.upper(), \n",
    "                              merged_df['predicted_label'].str.upper())\n",
    "    \n",
    "    print(f\"Matthews Correlation Coefficient (MCC) (multiclass): {mcc:0.2f}\")\n",
    "    print(f\"Cohen's Kappa (multiclass): {kappa:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store mcc\n",
    "%store kappa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
