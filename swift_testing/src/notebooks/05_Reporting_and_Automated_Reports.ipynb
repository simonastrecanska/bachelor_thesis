{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporting and Automated Reports\n",
    "\n",
    "This notebook logs key parameters, compiles computed metrics and visualizations, and exports them as an automated report (HTML). \n",
    "\n",
    "Before running this notebook, make sure that the earlier notebooks (e.g., Basic Analytics) have run so that key variables (such as `total_cases`, `accuracy`, etc.) are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine recreated from stored db_uri.\n",
      "Run Parameters:\n",
      "run_date: 2025-04-12 22:59:10\n",
      "model_version: 1.0.0\n",
      "num_test_cases: 99\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import text\n",
    "\n",
    "# Retrieve the stored connection string\n",
    "%store -r accuracy\n",
    "%store -r conf_stats\n",
    "%store -r db_uri\n",
    "%store -r kappa\n",
    "%store -r labels\n",
    "%store -r mc\n",
    "%store -r mcc\n",
    "%store -r merged_df\n",
    "%store -r min_confidence\n",
    "%store -r min_f1\n",
    "%store -r model_version\n",
    "%store -r report\n",
    "%store -r total_cases\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine(db_uri)\n",
    "print(\"Engine recreated from stored db_uri.\")\n",
    "\n",
    "# Check if key metrics exist, but do not assign default values\n",
    "try:\n",
    "    total_cases\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    accuracy\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    model_version\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "run_parameters = {\n",
    "    \"run_date\": datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    \"model_version\": model_version,\n",
    "    \"num_test_cases\": total_cases,\n",
    "    \"accuracy\": accuracy\n",
    "}\n",
    "\n",
    "print(\"Run Parameters:\")\n",
    "for key, value in run_parameters.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching results data from the database...\n"
     ]
    }
   ],
   "source": [
    "print(\"Fetching results data from the database...\")\n",
    "\n",
    "expected_query = text(\"\"\"\n",
    "SELECT message_id, expected_label \n",
    "FROM expected_results\n",
    "ORDER BY message_id\n",
    "\"\"\")\n",
    "\n",
    "actual_query = text(\"\"\"\n",
    "SELECT message_id, predicted_label, confidence \n",
    "FROM actual_results \n",
    "ORDER BY message_id\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    expected_df = pd.read_sql(expected_query, engine)\n",
    "    actual_df = pd.read_sql(actual_query, engine)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r mc\n",
    "%store -r mcc\n",
    "%store -r kappa\n",
    "%store -r report\n",
    "%store -r conf_stats\n",
    "%store -r labels\n",
    "%store -r min_f1\n",
    "%store -r min_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Assessment Analysis:\n",
      "- Overall Quality: EXCELLENT\n",
      "- Production Readiness: READY FOR PRODUCTION\n",
      "- Confidence in Results: MODERATE\n"
     ]
    }
   ],
   "source": [
    "def assess_model_quality():\n",
    "    \"\"\"Perform senior-level assessment of model quality based on metrics\"\"\"\n",
    "    # Define thresholds for different quality levels\n",
    "    quality_thresholds = {\n",
    "        'excellent': {'accuracy': 0.95, 'f1': 0.95, 'mcc': 0.9, 'kappa': 0.9, 'min_conf': 0.8},\n",
    "        'good': {'accuracy': 0.85, 'f1': 0.85, 'mcc': 0.7, 'kappa': 0.7, 'min_conf': 0.7},\n",
    "        'acceptable': {'accuracy': 0.75, 'f1': 0.75, 'mcc': 0.5, 'kappa': 0.5, 'min_conf': 0.6},\n",
    "        'needs_improvement': {'accuracy': 0.65, 'f1': 0.65, 'mcc': 0.3, 'kappa': 0.3, 'min_conf': 0.5}\n",
    "    }\n",
    "    \n",
    "    if (accuracy >= quality_thresholds['excellent']['accuracy'] and \n",
    "        min_f1 >= quality_thresholds['excellent']['f1'] and\n",
    "        mcc >= quality_thresholds['excellent']['mcc'] and\n",
    "        kappa >= quality_thresholds['excellent']['kappa'] and\n",
    "        min_confidence >= quality_thresholds['excellent']['min_conf']):\n",
    "        quality = \"EXCELLENT\"\n",
    "    elif (accuracy >= quality_thresholds['good']['accuracy'] and \n",
    "          min_f1 >= quality_thresholds['good']['f1'] and\n",
    "          mcc >= quality_thresholds['good']['mcc'] and\n",
    "          kappa >= quality_thresholds['good']['kappa'] and\n",
    "          min_confidence >= quality_thresholds['good']['min_conf']):\n",
    "        quality = \"GOOD\"\n",
    "    elif (accuracy >= quality_thresholds['acceptable']['accuracy'] and \n",
    "          min_f1 >= quality_thresholds['acceptable']['f1'] and\n",
    "          mcc >= quality_thresholds['acceptable']['mcc'] and\n",
    "          kappa >= quality_thresholds['acceptable']['kappa'] and\n",
    "          min_confidence >= quality_thresholds['acceptable']['min_conf']):\n",
    "        quality = \"ACCEPTABLE\"\n",
    "    elif (accuracy >= quality_thresholds['needs_improvement']['accuracy'] and \n",
    "          min_f1 >= quality_thresholds['needs_improvement']['f1'] and\n",
    "          mcc >= quality_thresholds['needs_improvement']['mcc'] and\n",
    "          kappa >= quality_thresholds['needs_improvement']['kappa'] and\n",
    "          min_confidence >= quality_thresholds['needs_improvement']['min_conf']):\n",
    "        quality = \"NEEDS IMPROVEMENT\"\n",
    "    else:\n",
    "        quality = \"POOR\"\n",
    "    \n",
    "    if quality in [\"EXCELLENT\", \"GOOD\"]:\n",
    "        readiness = \"READY FOR PRODUCTION\"\n",
    "    elif quality == \"ACCEPTABLE\":\n",
    "        readiness = \"POTENTIALLY READY WITH MONITORING\"\n",
    "    else:\n",
    "        readiness = \"NOT READY FOR PRODUCTION\"\n",
    "    \n",
    "    strengths = []\n",
    "    if accuracy >= 0.9:\n",
    "        strengths.append(f\"High overall accuracy ({accuracy:.2%})\")\n",
    "    if min_f1 >= 0.9:\n",
    "        strengths.append(\"Strong performance across all classes\")\n",
    "    if mcc >= 0.8:\n",
    "        strengths.append(\"Excellent correlation between predictions and ground truth\")\n",
    "    if min_confidence >= 0.8:\n",
    "        strengths.append(\"High confidence in predictions\")\n",
    "    \n",
    "    weaknesses = []\n",
    "    if accuracy < 0.8:\n",
    "        weaknesses.append(f\"Low overall accuracy ({accuracy:.2%})\")\n",
    "    if min_f1 < 0.8:\n",
    "        weaknesses.append(\"Inconsistent performance across classes\")\n",
    "    if mcc < 0.6:\n",
    "        weaknesses.append(\"Poor correlation between predictions and ground truth\")\n",
    "    if min_confidence < 0.7:\n",
    "        weaknesses.append(\"Low confidence in some predictions\")\n",
    "    \n",
    "    recommendations = []\n",
    "    if quality in [\"EXCELLENT\", \"GOOD\"]:\n",
    "        recommendations.append(\"Deploy model to production environment\")\n",
    "        recommendations.append(\"Implement regular monitoring to ensure continued performance\")\n",
    "    elif quality == \"ACCEPTABLE\":\n",
    "        recommendations.append(\"Consider deployment with enhanced monitoring\")\n",
    "        recommendations.append(\"Investigate classes with lower F1 scores for potential improvements\")\n",
    "        recommendations.append(\"Collect more training data for underperforming classes\")\n",
    "    else:\n",
    "        recommendations.append(\"Defer deployment until model quality improves\")\n",
    "        recommendations.append(\"Review training data for quality and representation issues\")\n",
    "        recommendations.append(\"Consider model architecture or hyperparameter tuning\")\n",
    "    \n",
    "    if len(merged_df) >= 100:\n",
    "        confidence = \"HIGH\"\n",
    "    elif len(merged_df) >= 50:\n",
    "        confidence = \"MODERATE\"\n",
    "    else:\n",
    "        confidence = \"LOW (limited test data)\"\n",
    "        recommendations.append(\"Increase test dataset size for more reliable evaluation\")\n",
    "    \n",
    "    return {\n",
    "        \"quality\": quality,\n",
    "        \"readiness\": readiness,\n",
    "        \"strengths\": strengths,\n",
    "        \"weaknesses\": weaknesses,\n",
    "        \"recommendations\": recommendations,\n",
    "        \"confidence\": confidence\n",
    "    }\n",
    "\n",
    "assessment = assess_model_quality()\n",
    "\n",
    "print(\"Model Assessment Analysis:\")\n",
    "print(f\"- Overall Quality: {assessment['quality']}\")\n",
    "print(f\"- Production Readiness: {assessment['readiness']}\")\n",
    "print(f\"- Confidence in Results: {assessment['confidence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated HTML report with detailed metrics and senior assessment.\n"
     ]
    }
   ],
   "source": [
    "def generate_detailed_report():\n",
    "    \"\"\"Generate a detailed HTML report with calculated metrics and assessment\"\"\"\n",
    "    html = \"<html>\\n\"\n",
    "    html += \"<head>\\n\"\n",
    "    html += \"<title>Test Run Report</title>\\n\"\n",
    "    html += \"<style>\\n\"\n",
    "    html += \"body { font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }\\n\"\n",
    "    html += \"h1 { color: #2c3e50; border-bottom: 1px solid #eee; padding-bottom: 10px; }\\n\"\n",
    "    html += \"h2 { color: #3498db; margin-top: 30px; }\\n\"\n",
    "    html += \"h3 { color: #2980b9; }\\n\"\n",
    "    html += \"table { border-collapse: collapse; width: 100%; margin: 20px 0; }\\n\"\n",
    "    html += \"th, td { padding: 12px; text-align: left; border: 1px solid #ddd; }\\n\"\n",
    "    html += \"th { background-color: #f2f2f2; }\\n\"\n",
    "    html += \"tr:hover { background-color: #f5f5f5; }\\n\"\n",
    "    html += \".metric-card { background: #f8f9fa; border-radius: 5px; padding: 15px; margin-bottom: 20px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }\\n\"\n",
    "    html += \".metric-value { font-size: 1.2em; font-weight: bold; color: #3498db; }\\n\"\n",
    "    html += \".excellent { color: #27ae60; font-weight: bold; }\\n\"\n",
    "    html += \".good { color: #2ecc71; font-weight: bold; }\\n\"\n",
    "    html += \".acceptable { color: #f39c12; font-weight: bold; }\\n\"\n",
    "    html += \".needs-improvement { color: #e67e22; font-weight: bold; }\\n\"\n",
    "    html += \".poor { color: #e74c3c; font-weight: bold; }\\n\"\n",
    "    html += \".strength { color: #27ae60; }\\n\"\n",
    "    html += \".weakness { color: #e74c3c; }\\n\"\n",
    "    html += \".recommendation { color: #3498db; }\\n\"\n",
    "    html += \"</style>\\n\"\n",
    "    html += \"</head>\\n\"\n",
    "    html += \"<body>\\n\"\n",
    "    \n",
    "    html += f\"<h1>Test Run Report - {run_parameters['run_date']}</h1>\\n\"\n",
    "    html += f\"<p><strong>Model Version:</strong> {run_parameters['model_version']}</p>\\n\"\n",
    "    html += f\"<p><strong>Number of Test Cases:</strong> {len(merged_df)}</p>\\n\"\n",
    "    html += f\"<p><strong>Accuracy:</strong> {accuracy:.2%}</p>\\n\"\n",
    "    \n",
    "    html += \"<h2>Model Quality Assessment</h2>\\n\"\n",
    "    html += \"<div class='metric-card'>\\n\"\n",
    "    \n",
    "    quality_class = assessment['quality'].lower().replace(' ', '-')\n",
    "    html += f\"<h3>Overall Quality: <span class='{quality_class}'>{assessment['quality']}</span></h3>\\n\"\n",
    "    html += f\"<p><strong>Production Readiness:</strong> {assessment['readiness']}</p>\\n\"\n",
    "    html += f\"<p><strong>Assessment Confidence:</strong> {assessment['confidence']}</p>\\n\"\n",
    "    \n",
    "    if assessment['strengths']:\n",
    "        html += \"<h4>Strengths:</h4>\\n\"\n",
    "        html += \"<ul>\\n\"\n",
    "        for strength in assessment['strengths']:\n",
    "            html += f\"<li class='strength'>{strength}</li>\\n\"\n",
    "        html += \"</ul>\\n\"\n",
    "    \n",
    "    if assessment['weaknesses']:\n",
    "        html += \"<h4>Areas for Improvement:</h4>\\n\"\n",
    "        html += \"<ul>\\n\"\n",
    "        for weakness in assessment['weaknesses']:\n",
    "            html += f\"<li class='weakness'>{weakness}</li>\\n\"\n",
    "        html += \"</ul>\\n\"\n",
    "    \n",
    "    html += \"<h4>Recommendations:</h4>\\n\"\n",
    "    html += \"<ul>\\n\"\n",
    "    for recommendation in assessment['recommendations']:\n",
    "        html += f\"<li class='recommendation'>{recommendation}</li>\\n\"\n",
    "    html += \"</ul>\\n\"\n",
    "    \n",
    "    html += \"</div>\\n\"\n",
    "    \n",
    "    html += \"<h2>Additional Metrics and Visualizations</h2>\\n\"\n",
    "    \n",
    "    html += \"<div class='metric-card'>\\n\"\n",
    "    html += \"<h3>Classification Report</h3>\\n\"\n",
    "    html += \"<table>\\n\"\n",
    "    html += \"<tr><th>Label</th><th>Precision</th><th>Recall</th><th>F1-Score</th><th>Support</th></tr>\\n\"\n",
    "    \n",
    "    lines = report.split('\\n')\n",
    "    for line in lines[2:-5]:\n",
    "        if line.strip():\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 5:\n",
    "                label = parts[0]\n",
    "                precision = float(parts[1])\n",
    "                recall = float(parts[2])\n",
    "                f1 = float(parts[3])\n",
    "                support = int(parts[4])\n",
    "                html += f\"<tr><td>{label}</td><td>{precision:.2f}</td><td>{recall:.2f}</td>\"\n",
    "                html += f\"<td>{f1:.2f}</td><td>{support}</td></tr>\\n\"\n",
    "    \n",
    "    html += \"</table>\\n\"\n",
    "    html += \"</div>\\n\"\n",
    "    \n",
    "    html += \"<div class='metric-card'>\\n\"\n",
    "    html += \"<h3>Confusion Matrix</h3>\\n\"\n",
    "    html += \"<table>\\n\"\n",
    "    \n",
    "    html += \"<tr><th></th>\"\n",
    "    for label in labels:\n",
    "        html += f\"<th>Predicted {label}</th>\"\n",
    "    html += \"</tr>\\n\"\n",
    "    \n",
    "    for i, true_label in enumerate(labels):\n",
    "        html += f\"<tr><td>Actual {true_label}</td>\"\n",
    "        for j in range(len(labels)):\n",
    "            html += f\"<td>{cm[i][j]}</td>\"\n",
    "        html += \"</tr>\\n\"\n",
    "    \n",
    "    html += \"</table>\\n\"\n",
    "    html += \"</div>\\n\"\n",
    "    \n",
    "    html += \"<div class='metric-card'>\\n\"\n",
    "    html += \"<h3>Advanced Evaluation Metrics</h3>\\n\"\n",
    "    html += \"<ul>\\n\"\n",
    "    html += f\"<li><strong>Matthews Correlation Coefficient:</strong> <span class='metric-value'>{mcc:.2f}</span></li>\\n\"\n",
    "    html += f\"<li><strong>Cohen's Kappa:</strong> <span class='metric-value'>{kappa:.2f}</span></li>\\n\"\n",
    "    html += f\"<li><strong>Balanced Accuracy:</strong> <span class='metric-value'>{accuracy:.2f}</span></li>\\n\"\n",
    "    html += \"</ul>\\n\"\n",
    "    html += \"</div>\\n\"\n",
    "    \n",
    "    html += \"<div class='metric-card'>\\n\"\n",
    "    html += \"<h3>Confidence Score Distribution</h3>\\n\"\n",
    "    html += \"<ul>\\n\"\n",
    "    for label, stats in conf_stats.items():\n",
    "        html += f\"<li><strong>{label}:</strong> Mean confidence = <span class='metric-value'>{stats['mean']:.2f}</span> (σ = {stats['std']:.2f})</li>\\n\"\n",
    "        html += f\"<ul><li>Range: {stats['min']:.2f} - {stats['max']:.2f}</li></ul>\\n\"\n",
    "    html += \"</ul>\\n\"\n",
    "    \n",
    "    high_conf_pct = (merged_df['confidence'] > 0.9).mean() * 100\n",
    "    html += f\"<p>{high_conf_pct:.1f}% of predictions have confidence scores above 0.90</p>\\n\"\n",
    "    html += \"</div>\\n\"\n",
    "        \n",
    "    html += \"</body>\\n\"\n",
    "    html += \"</html>\"\n",
    "    \n",
    "    return html\n",
    "\n",
    "report_html = generate_detailed_report()\n",
    "print(\"Generated HTML report with detailed metrics and senior assessment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated report saved as test_run_report_20250412_225918.html\n"
     ]
    }
   ],
   "source": [
    "# Save the report as an HTML file\n",
    "report_filename = f\"test_run_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.html\"\n",
    "with open(report_filename, 'w') as f:\n",
    "    f.write(report_html)\n",
    "\n",
    "print(f\"Automated report saved as {report_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The automated reporting system serves as the final component in the SWIFT message testing framework. Generates records of model performance so it enables comparison across versions to track improvements and detect regressions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
