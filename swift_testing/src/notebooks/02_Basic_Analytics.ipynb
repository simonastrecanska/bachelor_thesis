{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Analytics\n",
    "\n",
    "This notebook extracts and merges data from the database, computes standard evaluation metrics (accuracy, confusion matrix, and classification report), and performs some initial exploratory data analysis.\n",
    "\n",
    "It uses data from three tables:\n",
    "\n",
    "- **expected_results:** Contains the expected (target) routing information for each message.\n",
    "- **actual_results:** Contains the routing outcome predicted by your model.\n",
    "- **messages:** Contains the generated messages (the input texts).\n",
    "\n",
    "Ensure that the database connection has been set up (by running the `01_Database_Connection_Setup.ipynb` notebook) so that the engine variable is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from sqlalchemy import create_engine, text\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# Retrieve the stored connection string and recreate the engine\n",
    "%store -r db_uri\n",
    "engine = create_engine(db_uri)\n",
    "print(\"Engine created from db_uri.\")\n",
    "\n",
    "expected_df = pd.read_sql(\"SELECT * FROM expected_results\", engine)\n",
    "actual_df = pd.read_sql(\"SELECT * FROM actual_results\", engine)\n",
    "messages_df = pd.read_sql(\"SELECT * FROM messages\", engine)\n",
    "\n",
    "print(\"Expected Results Table:\")\n",
    "display(expected_df.head())\n",
    "\n",
    "print(\"Actual Results Table:\")\n",
    "display(actual_df.head())\n",
    "\n",
    "print(\"Messages Table (first 5 rows):\")\n",
    "display(messages_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Data and Computing Standard Metrics\n",
    "\n",
    "We will merge the three tables based on the key (`message_id`). The merge sequence is as follows:\n",
    "\n",
    "1. Merge **expected_results** and **actual_results** on `message_id`.\n",
    "2. Merge the result with the **messages** table to add the generated message text. (We assume the message text column in the messages table is named `generated_text`. If not, adjust accordingly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(expected_df, actual_df, on='message_id', how='inner')\n",
    "\n",
    "if 'generated_text' not in messages_df.columns:\n",
    "    print(\"Note: 'generated_text' column not found in messages_df. Check your messages table.\")\n",
    "else:\n",
    "    merged_df = pd.merge(merged_df, messages_df[['message_id', 'generated_text']], on='message_id', how='left')\n",
    "\n",
    "print(\"Merged Data (first 5 rows):\")\n",
    "display(merged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Metrics: Cases, Correct Predictions, and Accuracy\n",
    "\n",
    "In this section, we determine:\n",
    "\n",
    "- **`total_cases`**: The total number of rows in `merged_df` (i.e., the number of messages under analysis).\n",
    "- **`correct_predictions`**: How many predictions match their expected labels.\n",
    "- **`accuracy`**: The ratio of correct predictions to total cases, expressed as a percentage.\n",
    "\n",
    "Printing these values provides a high-level overview of the models performance:\n",
    "1. **Total cases** gives context on how many samples were tested.\n",
    "2. **Correct predictions** shows how often the model's guess aligns with reality.\n",
    "3. **Accuracy** reveals the overall success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['match'] = merged_df['expected_label'].str.upper() == merged_df['predicted_label'].str.upper()\n",
    "\n",
    "total_cases = merged_df.shape[0]\n",
    "correct_predictions = merged_df['match'].sum()\n",
    "accuracy = correct_predictions / total_cases if total_cases > 0 else 0\n",
    "\n",
    "print(f\"Total cases compared: {total_cases}\")\n",
    "print(f\"Correct predictions: {correct_predictions}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics and Confidence Statistics\n",
    "\n",
    "In this section, we perform an analysis of our model’s performance by:\n",
    "- **Confusion Matrix**: Displays the distribution of true versus predicted labels that helps us identify common misclassifications.\n",
    "- **Classification Report**: Provides per-class precision, recall, and F1 scores, that gave us a detailed view of the model’s performance on each label.\n",
    "- **Label Counts**: Shows the frequency of each expected label in the dataset.\n",
    "- **Confidence Statistics by Label**: For each predicted label, we calculate the mean, standard deviation, minimum, and maximum confidence scores. This can show us how confident the model is for each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_true = merged_df['expected_label'].str.upper()\n",
    "y_pred = merged_df['predicted_label'].str.upper()\n",
    "\n",
    "mc = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(mc)\n",
    "\n",
    "report = classification_report(y_true, y_pred, zero_division=0)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "labels = sorted(merged_df['expected_label'].unique())\n",
    "print(\"Label counts:\")\n",
    "print(merged_df['expected_label'].value_counts())\n",
    "print()\n",
    "\n",
    "conf_stats = {}\n",
    "for label in labels:\n",
    "    mask = merged_df['predicted_label'] == label\n",
    "    conf_values = merged_df.loc[mask, 'confidence']\n",
    "    conf_stats[label] = {\n",
    "        'mean': conf_values.mean() if len(conf_values) > 0 else 0,\n",
    "        'std': conf_values.std() if len(conf_values) > 0 else 0,\n",
    "        'min': conf_values.min() if len(conf_values) > 0 else 0,\n",
    "        'max': conf_values.max() if len(conf_values) > 0 else 0\n",
    "    }\n",
    "\n",
    "class_f1_scores = {}\n",
    "lines = report.split('\\n')\n",
    "for line in lines[2:-5]:\n",
    "    if line.strip():\n",
    "        parts = line.split()\n",
    "        if len(parts) >= 5:\n",
    "            label = parts[0]\n",
    "            f1 = float(parts[3])\n",
    "            class_f1_scores[label] = f1\n",
    "\n",
    "min_f1 = min(class_f1_scores.values()) if class_f1_scores else 0\n",
    "\n",
    "min_confidence = merged_df['confidence'].min()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we calculate the distribution of model predictions. The match column is a boolean value that indicates whether the predicted label matches the expected label (True for correct predictions, False for incorrect ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prediction_counts = merged_df['match'].value_counts()\n",
    "print(\"\\nPrediction counts:\")\n",
    "print(prediction_counts)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "prediction_counts.plot(kind='bar')\n",
    "plt.title('Correct vs Incorrect Predictions')\n",
    "plt.xlabel('Prediction Correct? (True = Correct)')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the merged DataFrame for use in subsequent notebooks\n",
    "%store merged_df\n",
    "\n",
    "%store accuracy\n",
    "%store mc\n",
    "%store conf_stats\n",
    "%store labels\n",
    "%store merged_df\n",
    "%store min_confidence\n",
    "%store min_f1\n",
    "%store report\n",
    "%store total_cases\n",
    "\n",
    "# Please update version of your model\n",
    "model_version = \"1.0.0\"\n",
    "%store model_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has:\n",
    "\n",
    "- Loaded data from the **expected_results**, **actual_results**, and **messages** tables.\n",
    "- Merged them using the common key (`message_id`) to include the generated message text.\n",
    "- Computed standard metrics (accuracy, confusion matrix, and a classification report).\n",
    "- Performed initial exploratory data analysis (e.g., a bar plot of correct vs. incorrect predictions).\n",
    "\n",
    "The merged DataFrame, `merged_df`, is stored for use in subsequent analysis notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
